/**
 * sample-enhance-then-crop.ts
 *
 * Requirements:
 *  - Node 18+ (for global fetch) or remove the fetch fallback.
 *  - npm i sharp tesseract.js openai cloudinary
 *  - set env vars: CLOUDINARY_* or edit config below, and OPENAI_API_KEY
 *
 * Run:
 *  npx ts-node sample-enhance-then-crop.ts
 */

import { NextRequest, NextResponse } from 'next/server';
import Tesseract from "tesseract.js";
import { PrismaClient } from '@prisma/client';


import sharp from "sharp";
import { exec } from 'child_process';
import { writeFile, readFile, unlink } from 'fs/promises';
import { tmpdir } from 'os';
import { v2 as cloudinary } from "cloudinary";
import OpenAI from "openai";
import { uploadToCloudinary } from '../../utils/cloudinary';
// import processImageUrl from '@/sample1';



const prisma = new PrismaClient();


cloudinary.config({
  cloud_name: process.env.CLOUDINARY_CLOUD_NAME,
  api_key: process.env.CLOUDINARY_API_KEY,
  api_secret: process.env.CLOUDINARY_API_SECRET,
});

// --- OpenAI client (needs OPENAI_API_KEY in env) ---
// const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY || "" });
const openai = new OpenAI({
  apiKey:process.env.OPENAI_API_KEY
});

// Fallback for fetch in older Node (attempt dynamic import if needed)
const fetchFn: typeof fetch =
  (globalThis as any).fetch ||
  (async (...args: any[]) => {
    const mod = await import("node-fetch");
    return (mod.default as any)(...args);
  }) as any;

interface ImagePart {
  left: number;
  top: number;
  width: number;
  height: number;
  type: string;
  position: number;
}

/**
 * Enhance the entire image buffer first, then return the enhanced buffer.
 * Uses modulate + linear + sharpen to simulate brightness/contrast and clarity.
 */
async function enhanceFullImage(buffer: Buffer): Promise<Buffer> {
  return await sharp(buffer)
    .modulate({ brightness: 1.12, saturation: 1.05 }) // brightness + slight saturation
    .linear(1.07, -6) // approximate contrast: (a * x + b)
    .sharpen()
    .toBuffer();
}

/**
 * Basic AI cleanup of OCR text using OpenAI (gives back numbered cleaned questions)
 */
async function enhanceQuestionsWithAI(questions: string[]): Promise<string[]> {
  const joinedQuestions = questions.join("\n");

  const prompt = `
You are a helpful assistant that cleans and formats OCR text into readable Hindi/English questions.

Input text (may contain noise):
${joinedQuestions}

Output the cleaned, well-formed list of questions in correct language (Hindi or English as detected):
`;

  const response = await openai.chat.completions.create({
    model: "gpt-4o-mini",
    messages: [{ role: "user", content: prompt }],
  });

  const aiOutput = response.choices[0].message.content || "";
  return aiOutput
    .split("\n")
    .map(q => q.trim())
    .filter(q => q.length > 0);
}

/**
 * Main pipeline:
 * 1) fetch image
 * 2) enhance full image
 * 3) compute dynamic parts (top region -> 2x2 grid, bottom -> text area)
 * 4) crop enhanced buffer for each part, upload to Cloudinary
 * 5) OCR bottom enhanced crop and clean with OpenAI
 */
async function processImageUrl(imageUrl: string ,screenId:string) {
  try {
    console.log("üì• Fetching image:", imageUrl);
    const resp = await fetchFn(imageUrl);
    if (!resp.ok) throw new Error(`Failed to fetch image: ${resp.status} ${resp.statusText}`);
    const arrayBuffer = await resp.arrayBuffer();
    const inputBuffer = Buffer.from(arrayBuffer);

    console.log("‚ú® Enhancing full image (this will be used as source for all crops)...");
    const enhancedFull = await enhanceFullImage(inputBuffer);

    // Get enhanced image metadata (width/height)
    const meta = await sharp(enhancedFull).metadata();
    const width = meta.width || 0;
    const height = meta.height || 0;
    if (!width || !height) throw new Error("Could not determine image dimensions.");

    console.log(`üìê Enhanced image dimensions: ${width}x${height}`);

    // Decide layout:
    // - Top region: roughly top 70% (objects) -> split into 2x2 grid
    // - Bottom region: remaining ~30% for questions (you can tweak ratio)
    const topRatio = 0.70;
    const topHeight = Math.floor(height * topRatio);
    const bottomHeight = height - topHeight;
    const halfWidth = Math.floor(width / 2);
    const halfTopHeight = Math.floor(topHeight / 2);

    // build parts dynamically, based on actual size
    const objectParts: ImagePart[] = [
      { left: 0, top: 0, width: halfWidth, height: halfTopHeight, type: "obj_1", position: 1 },
      { left: halfWidth, top: 0, width: width - halfWidth, height: halfTopHeight, type: "obj_2", position: 2 },
      { left: 0, top: halfTopHeight, width: halfWidth, height: topHeight - halfTopHeight, type: "obj_3", position: 3 },
      { left: halfWidth, top: halfTopHeight, width: width - halfWidth, height: topHeight - halfTopHeight, type: "obj_4", position: 4 },
    ];

    // Crop each object from the ENHANCED full image and upload
    const uploadedImageUrls: string[] = [];
    for (const part of objectParts) {
      try {
        const cropBuf = await sharp(enhancedFull)
          .extract({ left: part.left, top: part.top, width: part.width, height: part.height })
          .resize({ width: Math.min(part.width, 1024) }) // optional resize for upload
          .png()
          .toBuffer();

        // Optional further local processing on each crop (if needed) could go here.

        const up = await cloudinary.uploader.upload(
          `data:image/png;base64,${cropBuf.toString("base64")}`,
          { folder: "split_parts_enhanced", public_id: `${part.type}_${Date.now()}`, resource_type: "image" }
        );
        uploadedImageUrls.push(up.secure_url);
        console.log(`‚úÖ Uploaded part ${part.position} -> ${up.secure_url}`);
        // Save to database
        const image = await prisma.image.create({
          data: {
            url: up.secure_url,
            screenId:parseInt(screenId),
          },
        });
   
      } catch (err) {
        console.error(`‚ùå Error cropping/uploading part ${part.position}:`, err);
      }
    }

    // Crop bottom area from ENHANCED image for OCR (ensures OCR sees enhanced text)
    console.log("üîé Preparing bottom region for OCR...");
    const bottomCropBuf = await sharp(enhancedFull)
      .extract({ left: 0, top: topHeight, width: width, height: bottomHeight })
      .resize({ width: Math.min(width, 1600) }) // resize for better OCR if huge
      .png()
      .toBuffer();

    // (Optional) upload bottom cropped image too (for debugging/preview)
    try {
      const upBottom = await cloudinary.uploader.upload(
        `data:image/png;base64,${bottomCropBuf.toString("base64")}`,
        { folder: "split_parts_enhanced", public_id: `bottom_${Date.now()}`, resource_type: "image" }
      );
      console.log("‚úÖ Uploaded bottom enhanced crop:", upBottom.secure_url);
    } catch (err) {
      console.warn("‚ö†Ô∏è Failed to upload bottom crop (non-fatal):", err);
    }

    console.log("üî† Running OCR on full image (hin+eng)...");
           const result: any = await Tesseract.recognize(inputBuffer, "hin+eng");
           const text = result?.data?.text || "";
       
           const questions = text
             .split("\n")
             .map((q: string) => q.trim())
             .filter((q: string) => q.length > 0);
       
           console.log("\nüñºÔ∏è Cropped image URLs:");
           uploadedImageUrls.forEach((u) => console.log(" -", u));
       
           console.log("\nüìñ Extracted Questions:");
           function cleanQuestions(questions: string[]): string[] {
               return questions
                   .map(q =>
                   q
                       .replace(/[^\p{L}\p{N}\s?‡•§?!]/gu, "") // remove symbols but keep Hindi/English chars
                       .replace(/\s+/g, " ") // normalize spaces
                       .trim()
                   )
                   .filter(q => q.length > 3); // remove too-short noise
               }
        
           const cleaned = await enhanceQuestionsWithAI(questions);
       
       // Remove duplicate numbering like "1. 1."
       const finalQuestions = cleaned.map(q => q.replace(/^\d+\.\s*/, '').trim());
       
       console.log("\n‚ú® Final Enhanced Questions:");
       finalQuestions.forEach((q, i) => console.log(`${i + 1}. ${q}`));

    return { uploadedImageUrls, finalQuestions };
  } catch (err) {
    console.error("üö® Error in processImageUrl:", err);
    throw err;
  }
}


/**
 * Main POST handler
 */
export async function POST(request: NextRequest) {
  try {
    const formData = await request.formData();
    const screenId = formData.get('screenId') as string;
    const file = formData.get('file') as File | null;
    const imageUrl = formData.get('imageUrl') as string | null;

    if (!screenId) {
      return NextResponse.json({ error: 'Screen ID is required' }, { status: 400 });
    }

    const screenIdInt = parseInt(screenId);
    if (isNaN(screenIdInt)) {
      return NextResponse.json({ error: 'Invalid screen ID' }, { status: 400 });
    }

    // Delete existing data for the screen in correct order to avoid foreign key constraints
    await prisma.userResponse.deleteMany({
      where: {
        question: {
          screenId: screenIdInt
        }
      }
    });
    await prisma.question.deleteMany({ where: { screenId: screenIdInt } });
    await prisma.image.deleteMany({ where: { screenId: screenIdInt } });

    let cloudinaryUrl: string;

    if (file) {
      // Upload new file
      const fileBuffer = Buffer.from(await file.arrayBuffer());
      cloudinaryUrl = await uploadToCloudinary(fileBuffer, 'patient-test-images');
    } else if (imageUrl) {
      cloudinaryUrl = imageUrl;
    } else {
      return NextResponse.json({ error: 'Either file or imageUrl must be provided' }, { status: 400 });
    }

    const result = await processImageUrl(cloudinaryUrl,screenId);

    // Save extracted questions to DB
    for (const questionText of result.finalQuestions) {
      await prisma.question.create({
        data: {
          text: questionText,
          screenId: screenIdInt,
        },
      });
    }

    

    return NextResponse.json({
      success: true,
      uploadedImageUrls: result.uploadedImageUrls,
      questions: result.finalQuestions,
    });
   

  } catch (error) {
    console.error('‚ùå Error in POST /api/upload-split:', error);
    return NextResponse.json({ error: 'Internal server error' }, { status: 500 });
  }
}
